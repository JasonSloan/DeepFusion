#  ==以下封装过程是个有先后包含顺序递进的过程==

# 一. 模型编译封装（任务40）

##1. simple-logger.cpp和simple_logger.hpp

==封装了日志打印相关==

### （1）日志等级

```C++
Debug   = 5,
Verbose = 4,
Info    = 3,
Warning = 2,
Error   = 1,
Fatal   = 0
```

### （2）提供接口

```C++
void set_log_level(LogLevel level);  	// 设置日志等级
LogLevel get_log_level();				// 获得日志等级
```

### （3）示例

```C++
SimpleLogger::set_log_level(SimpleLogger::LogLevel::Verbose);	// 设置日志等级为verbose
```

## 2. cuda-tools.hpp和cuda-tools.cpp

==封装了cuda-runtime-api相关的检查==

### （1）提供接口

```
bool check_runtime(cudaError_t e, const char* call, int iLine, const char *szFile);  //检查cuda-runtime-api执行成功与否的包装
bool check_device_id(int device_id);    // 检查设备ID（一般不用）
int current_device_id();    // 获得当前设备ID（一般不用）
```

##3. trt_builder.hpp和trt_builder.cpp 

==封装了模型tensorrt模型构建的pipeline==

### （1）支持数据类型

```C+=
FP32,
FP16
```

### （2）提供接口

```C++
bool compile(
  Mode mode,
  unsigned int maxBatchSize,
  const std::string& source,
  const std::string& saveto,
  const size_t maxWorkspaceSize = 1ul << 30                // 1ul << 30 = 1GB
);		// 提供从onnx模型到编译成为tensorrt模型的接口
```

## （3）示例

```C++
TRT::compile(
  TRT::Mode::FP32,
  10,
  "classifier.onnx",
  "engine.trtmodel",
  1 << 28
);
```

# 二. 内存管理封装（任务41）

==封装了在cpu和gpu上开辟内存==

## 1. mix-memory.cpp和mix-memory.hpp

### （1）提供接口

```C++
void* gpu(size_t size);		// 在gpu上分配内存
void* cpu(size_t size);		// 在cpu上分配内存
```

### （2）示例

```C++
int input_numel = input_batch * input_channel * input_height * input_width;		
MixMemory input_data;															// 实例化一个MixMemory实例对象
float* input_data_host   = input_data.cpu<float>(input_numel);					// 在cpu上分配内存
float* input_data_device = input_data.gpu<float>(input_numel);					// 在gpu上分配内存
```

# 三. 数据封装成tensor（任务42）

## 1. trt-tensor.cpp和trt-tensor.hpp

==封装了以下几点==

```C++
// 1.内存的管理(在cpu或者gpu上开辟空间)，mixmemory解决
// 2.内存的复用(在cpu或者gpu上开辟内存的复用），mixmemory解决
// 3.内存的copy，比如说cpu-》GPU，GPU-》cpu
//		解决方案 (从caffe上学到的思路) :
//			a. 定义内存的状态，表示当前最新的内容在哪里 (GPU/CPU/Init)
//			b. 懒分配原则，当你需要使用时，才会考虑分配内存
//			c. 获取内存地址，即表示: 想拿到最新的数据比如说 tensor.cpu表示我想拿到最新的数据，并且把它放到cpu上
//				比如说 tensor.gpu表示我想拿到最新的数据，并且把它放到gpu上
// 4.索引的计算，比如说,我有5d的tensor(B,D,C，H,W)，此时我要获取B = 1,D = 3, C = 0，H = 5,W = 9的位置元素(也就是计算tensor中某个位置的元素的地址相对于首地址的偏移量)
```

### (1)提供接口

```C++
        // 几种创建tensor的方式：最常用②根据数据形状创建
        explicit Tensor(DataType dtype = DataType::Float, std::shared_ptr<MixMemory> data = nullptr, int device_id = CURRENT_DEVICE_ID);
        explicit Tensor(int n, int c, int h, int w, DataType dtype = DataType::Float, std::shared_ptr<MixMemory> data = nullptr, int device_id = CURRENT_DEVICE_ID);
        explicit Tensor(int ndims, const int* dims, DataType dtype = DataType::Float, std::shared_ptr<MixMemory> data = nullptr, int device_id = CURRENT_DEVICE_ID);
        explicit Tensor(const std::vector<int>& dims, DataType dtype = DataType::Float, std::shared_ptr<MixMemory> data = nullptr, int device_id = CURRENT_DEVICE_ID);
        virtual ~Tensor();
        // tensor的一些属性
        int numel() const;
        inline int ndims() const{return shape_.size();}
        inline int size(int index)  const{return shape_[index];}
        inline int shape(int index) const{return shape_[index];}
        // tensor的一些属性
        inline int batch()   const{return shape_[0];}
        inline int channel() const{return shape_[1];}
        inline int height()  const{return shape_[2];}
        inline int width()   const{return shape_[3];}
        // tensor的一些属性
        inline DataType type()                const { return dtype_; }
        inline const std::vector<int>& dims() const { return shape_; }
        inline const std::vector<size_t>& strides() const {return strides_;}
        inline int bytes()                    const { return bytes_; }
        inline int bytes(int start_axis)      const { return count(start_axis) * element_size(); }
        inline int element_size()             const { return data_type_size(dtype_); }
        inline DataHead head()                const { return head_; }
        // tensor的一些常用方法
        std::shared_ptr<Tensor> clone() const;
        Tensor& release();
        Tensor& set_to(float value);
        bool empty() const;
        // 计算tensor某个元素相对于首地址的偏移量的函数
        template<typename ... _Args>
        int offset(int index, _Args ... index_args) const{      
            const int index_array[] = {index, index_args...};
            return offset_array(sizeof...(index_args) + 1, index_array);
        }

        int offset_array(const std::vector<int>& index) const;
        int offset_array(size_t size, const int* index_array) const;
        // tensor的resize方法
        template<typename ... _Args>
        Tensor& resize(int dim_size, _Args ... dim_size_args){
            const int dim_size_array[] = {dim_size, dim_size_args...};
            return resize(sizeof...(dim_size_args) + 1, dim_size_array);
        }
        Tensor& resize(int ndims, const int* dims);
        Tensor& resize(const std::vector<int>& dims);
        Tensor& resize_single_dim(int idim, int size);
        int  count(int start_axis = 0) const;
        int device() const{return device_id_;}
        // 将tensor的数据转移到cpu或者gpu上，返回原tensor数据（可以不要返回值）
        Tensor& to_gpu(bool copy=true);
        Tensor& to_cpu(bool copy=true);
        // 将tensor的数据转移到cpu或者gpu上，且返回转移后的内存（显存）地址
        inline void* cpu() const { ((Tensor*)this)->to_cpu(); return data_->cpu(); }
        inline void* gpu() const { ((Tensor*)this)->to_gpu(); return data_->gpu(); }
        
        template<typename DType> inline const DType* cpu() const { return (DType*)cpu(); }
        template<typename DType> inline DType* cpu()             { return (DType*)cpu(); }
        // 接受偏移量作为参数，返回偏移后的地址（具体见示例）
        template<typename DType, typename ... _Args> 
        inline DType* cpu(int i, _Args&& ... args) { return cpu<DType>() + offset(i, args...); }


        template<typename DType> inline const DType* gpu() const { return (DType*)gpu(); }
        template<typename DType> inline DType* gpu()             { return (DType*)gpu(); }

        template<typename DType, typename ... _Args> 
        inline DType* gpu(int i, _Args&& ... args) { return gpu<DType>() + offset(i, args...); }
```

### （2）示例

```C++
void inference(){

    TRTLogger logger;
    auto engine_data = load_file("engine.trtmodel");
    auto runtime   = make_nvshared(nvinfer1::createInferRuntime(logger));
    auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));
    if(engine == nullptr){
        printf("Deserialize cuda engine failed.\n");
        runtime->destroy();
        return;
    }

    cudaStream_t stream = nullptr;
    checkRuntime(cudaStreamCreate(&stream));
    auto execution_context = make_nvshared(engine->createExecutionContext());

    int input_batch   = 1;
    int input_channel = 3;
    int input_height  = 224;
    int input_width   = 224;
    int input_numel   = input_batch * input_channel * input_height * input_width;

    // tensor的建立并不会立即分配内存，而是在第一次需要使用的时候进行分配
    TRT::Tensor input_data({input_batch, input_channel, input_height, input_width}, TRT::DataType::Float);

    // 为input关联stream，使得在同一个pipeline中执行复制操作
    input_data.set_stream(stream);

    ///////////////////////////////////////////////////
    // image to float
    auto image = cv::imread("dog.jpg");
    float mean[] = {0.406, 0.456, 0.485};
    float std[]  = {0.225, 0.224, 0.229};

    // 对应于pytorch的代码部分
    cv::resize(image, image, cv::Size(input_width, input_height));
    image.convertTo(image, CV_32F);

    // 利用opencv mat的内存地址引用，实现input与mat的关联，然后利用split函数一次性完成mat到input的复制
    cv::Mat channel_based[3];
    for(int i = 0; i < 3; ++i)
        // 注意这里 2 - i是实现bgr -> rgb的方式
        // 这里cpu提供的参数0是表示batch的索引是0，第二个参数表示通道的索引，因此获取的是0, 2-i通道的地址
        // 而tensor最大的好处就是帮忙计算索引，否则手动计算就得写很多代码
        channel_based[i] = cv::Mat(input_height, input_width, CV_32F, input_data.cpu<float>(0, 2-i));

    cv::split(image, channel_based);

    // 利用opencv的mat操作加速减去均值和除以标准差
    for(int i = 0; i < 3; ++i)
        channel_based[i] = (channel_based[i] / 255.0f - mean[i]) / std[i];
    
    // 如果不写，input_data.gpu获取gpu地址时会自动进行复制
    // 目的就是把内存复制变为隐式进行
    input_data.to_gpu();

    // 3x3输入，对应3x3输出
    const int num_classes = 1000;
    TRT::Tensor output_data({input_batch, num_classes}, TRT::DataType::Float);
    output_data.set_stream(stream);

    // 明确当前推理时，使用的数据输入大小
    auto input_dims = execution_context->getBindingDimensions(0);
    input_dims.d[0] = input_batch;

    execution_context->setBindingDimensions(0, input_dims);
    float* bindings[] = {input_data.gpu<float>(), output_data.gpu<float>()};
    bool success      = execution_context->enqueueV2((void**)bindings, stream, nullptr);
    checkRuntime(cudaStreamSynchronize(stream));

    // 当获取cpu地址的时候，如果数据最新的在gpu上，就进行数据复制，然后再返回cpu地址
    float* prob = output_data.cpu<float>();
    int predict_label = std::max_element(prob, prob + num_classes) - prob;
    auto labels = load_labels("labels.imagenet.txt");
    auto predict_name = labels[predict_label];
    float confidence  = prob[predict_label];
    printf("Predict: %s, confidence = %f, label = %d\n", predict_name.c_str(), confidence, predict_label);

    checkRuntime(cudaStreamDestroy(stream));
}
```

# 四. 封装inference（任务43）

## 1. trt-infer.cpp和trt-infer.hpp

### （1）提供接口

```C++
		virtual void forward(bool sync = true) = 0; 					// 只用这个接口
		virtual int get_max_batch_size() = 0;
		virtual void set_stream(CUStream stream) = 0;
		virtual CUStream get_stream() = 0;
		virtual void synchronize() = 0;
		virtual size_t get_device_memory_size() = 0;
		virtual std::shared_ptr<MixMemory> get_workspace() = 0;
		virtual std::shared_ptr<Tensor> input(int index = 0) = 0;  		// 和 这个接口
		virtual std::shared_ptr<Tensor> output(int index = 0) = 0; 		// 和 这个接口
		virtual std::shared_ptr<Tensor> tensor(const std::string &name) = 0;
		virtual std::string get_input_name(int index = 0) = 0;
		virtual std::string get_output_name(int index = 0) = 0;
		virtual bool is_output_name(const std::string &name) = 0;
		virtual bool is_input_name(const std::string &name) = 0;
		virtual int num_output() = 0;
		virtual int num_input() = 0;
		virtual void print() = 0;
		virtual int device() = 0;
		virtual void set_input(int index, std::shared_ptr<Tensor> tensor) = 0;
		virtual void set_output(int index, std::shared_ptr<Tensor> tensor) = 0;
		virtual std::shared_ptr<std::vector<uint8_t>> serial_engine() = 0;
```

### （2）示例（这个inference代码比上面那个简化了不少）

```C++
void inference(){

    auto engine = TRT::load_infer("engine.trtmodel");
    if(engine == nullptr){
        printf("Deserialize cuda engine failed.\n");
        return;
    }

    engine->print();

    auto input       = engine->input();
    auto output      = engine->output();
    int input_width  = input->width();
    int input_height = input->height();

    ///////////////////////////////////////////////////
    // image to float
    auto image = cv::imread("dog.jpg");
    float mean[] = {0.406, 0.456, 0.485};
    float std[]  = {0.225, 0.224, 0.229};

    // 对应于pytorch的代码部分
    cv::resize(image, image, cv::Size(input_width, input_height));
    image.convertTo(image, CV_32F);

    cv::Mat channel_based[3];
    for(int i = 0; i < 3; ++i)
        channel_based[i] = cv::Mat(input_height, input_width, CV_32F, input->cpu<float>(0, 2-i));

    cv::split(image, channel_based);
    for(int i = 0; i < 3; ++i)
        channel_based[i] = (channel_based[i] / 255.0f - mean[i]) / std[i];

    engine->forward(true);    

    int num_classes   = output->size(1);
    float* prob       = output->cpu<float>();
    int predict_label = std::max_element(prob, prob + num_classes) - prob;
    auto labels       = load_labels("labels.imagenet.txt");
    auto predict_name = labels[predict_label];
    float confidence  = prob[predict_label];
    printf("Predict: %s, confidence = %f, label = %d\n", predict_name.c_str(), confidence, predict_label);
}
```

