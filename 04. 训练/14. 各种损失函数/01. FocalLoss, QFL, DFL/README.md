## 一. FL(Focal Loss)

Focal Loss 的引入主要是为了解决难易样本数量不平衡（注意, 有区别于正负样本数量不平衡）的问题, 实际可以使用的范围非常广泛。比如单阶段的目标检测器通常会产生高达 100 k 的候选目标，只有极少数是正样本，正负样本数量非常不平衡。在计算分类的时候常用的损失一一交叉熵的公式如下:

![](assets/fl1.jpg)

为了解决正负样本不平衡的问题，通常会在交叉熵损失的前面加上一个参数 α，即：

![](assets/fl2.jpg)

但这无法解决全部问题。根据正、负、难、易, 样本一共可以分为以下四类：

![](assets/fl3.jpg)

尽管 α 平衡了正负样本， 但对难易样本的不平衡没有任何帮助。而实际上，目标检测中大量的候选目标都是像下图一样的易分样本。

![](assets/fl4.jpg)

这些样本损失很低，但由于数量极不平衡，易分样本的数量相对来讲太多，最终主导了总的损失。而Focal loss的作者认为，易分样本（即置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本, 这时候, Focal Loss 就上场了。

一个简单的思想：把高置信度p样本的损失再降低一些不就好了吗。

![](assets/fl5.jpg)

举例， r 取 2 时, 如果 p = 0.968 , ( 1 − 0.968 ) ^2 ≈ 0.001 损失衰减了 1000 倍!

![](assets/fl6.jpg)

 实验表明r 取 2 , alpha 取 0.25的时候效果最佳

## 二. QFL

参考:https://zhuanlan.zhihu.com/p/147691786

## 三. DFL

参考:https://zhuanlan.zhihu.com/p/147691786

**训练阶段**:  **TODO**

**推理阶段**: 模型在预测边框的大小时, 不再预测xywh而是预测ltrb(left top right bottom), 也就是预测最后大中小三个特征图中每个单元格中心点到边框的四个边的距离, 而这个距离也不是直接预测的, 而是预测16(16这个数可调)个值。

这16个值代表什么意思呢? 代表某一侧的边框距离单元格中心点0、1、2、3......个像素位置的16个概率, 将这16个概率中的每一个概率乘以对应位置, 就得到了预测边框距离单元格中心点的预测距离。

```python
# 伪代码
reg = torch.randn([16]).softmax(0)		# 16个位置的概率分布(和为1)
indexes = torch.arange(16)			    # 16个位置的索引
dist = torch.sum(reg * indexes)			# 概率乘以索引求和即是某一边距离单元格中心点的预测值
```





