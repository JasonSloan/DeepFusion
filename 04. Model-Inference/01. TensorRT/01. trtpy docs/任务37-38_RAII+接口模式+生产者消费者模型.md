完整代码

trtpy get-series tensorrt-integrate

cd tensorrt-integrate

trtpy change-proj 1.12

# 一. 生产者消费者模型（任务37）

![](pics/生产者消费者模型.png)

```c++
#include<thread>//线程
#include<queue>//队列
#include<mutex>//线程锁
#include<string>
#include<chrono>//时间库
#include <memory>//智能指针
#include<future>  //future和promise都在这个库里，实现线程间数据传输
#include<condition_variable>// 线程通信库
const int limit_ = 10;
using namespace std;
struct Job{
    shared_ptr<promise<string>> pro;//为了实现线程间数据的传输，需要定义一个promise，由智能指针托管
    string input;
};

queue<Job> qjobs_;//初始化一个队列，队列中存放Job结构体
mutex lock_;
condition_variable cv_;
// 生产者，一直生产图片，然后分发给各个消费者，然后将消费者的结果回收再统一处理（想想自动驾驶大模型，一个图片既要做目标检测，又要做语义分割）
void video_capture(){
    Job job;
    int pic_id = 0;
    while(true){
        {   //这里花括号保证上锁的作用域在对数据操作的部分
            unique_lock<mutex> l(lock_);//unique_lock唯一锁。（ps：lock_guard互斥锁，和智能指针很像，这样就不用自己主动释放了）
            char name[100];
            sprintf(name, "PIC-%d", pic_id++);//相当于python中的format，生成一个format字符串然后赋值给name
            job.pro.reset(new promise<string>);
            job.input = name;
            qjobs_.push(job);
            printf("生产了一个新图片：%s, qjobs_.size=%d\n", name, qjobs_.size());
            // 生产者端：如果队列满了，就等待一会；消费者端：如果消费掉一个，就通知生产者可以继续了
            // 线程通信
            // cv_.wait() 函数接受一个 std::unique_lock 作为参数，然后会释放这个锁并使当前线程进入睡眠状态，
            // 等待其他线程通知它醒来。一旦收到其他线程的通知，或者发生伪唤醒，wait() 函数会重新尝试获取锁，然后返回
            cv_.wait(l, [&](){
                return qjobs_.size() < limit_;
            });// condition_variable常用用法，return false代表需要继续等，return true 代表要退出
        }
        // .get过后，实现等待，直到在消费者中promise->set_value被执行，这一句才会真的被执行获得result
        auto result = job.pro->get_future().get();
        printf("Job %s -> %s\n", job.input.c_str(), result.c_str());
        this_thread::sleep_for(chrono::milliseconds(500));//this_thread是std标准库的命名空间，可以直接对当前线程操作。更多用法可以查询gpt
    }
    this_thread::yield();//this_thread的常用用法，一般都会加上这个，当线程空闲时，放弃对处理器的控制

}


// 消费者，一直推理图片
void infer_worker(){
    while(true){
        if(!qjobs_.empty()){
            {
                lock_guard<mutex> l(lock_);//lock_guard互斥锁，和智能指针很像，这样就不用自己主动释放了
                auto pjob = qjobs_.front();//从队列中取元素进行推理
                qjobs_.pop();
                printf("推理一个图片：%s\n",pjob.input.c_str());
                cv_.notify_all();// 消费掉一个图片，就赶紧通知生产者可以继续生产了
                auto result = pjob.input + "-->infered";//相当于模型执行推理的过程
                pjob.pro->set_value(result);// 将模型推理的结果放入prjob的promise中
            }
            this_thread::sleep_for(chrono::milliseconds(1000));//this_thread是std标准库的命名空间，可以直接对当前线程操作。更多用法可以查询gpt
        }

    }

}

int main (){
    thread t0(video_capture);//生产者线程
    thread t1(infer_worker);//消费者线程
    t0.join();//主线程等待子线程结束
    t1.join();//主线程等待子线程结束
}

```

# 二. RAII+接口模式（任务38）

在一个推理文件中有一个Infer类，Infer类必须实现load_model，forward，destroy等成员方法，但是使用者在实例化Infer类的时候不知道怎么使用，可能没有load_model就先forward了，就会容易造成错误，而如果为了避免使用者出现错误在Infer类中加很多判断逻辑（比如forward中增加模型是否加载成功的判断），那么代码就会非常冗余。

如何解决这个问题，就是使用RAII+接口模式。先实现一个虚基类，然后让Infer继承该虚基类，然后再实现一个create_infer函数，该函数内部new一个Infer类，在函数内部实现模型是否加载的判断，然后将new出来的Infer类的指针向虚基类转化，这样，给用户只提供这个create_infer的函数，用户得到的指针只能调用forward函数，从而避免错误。

main.cpp文件：

```C++
#include <stdio.h>
#include "infer.hpp"
using namespace std;

int main()
{
    auto infer = create_infer("a"); // 这里"a"代表模型路径
    if (infer == nullptr)           // 调用者只需要判断指针是否为空即可
    {
        printf("load model failed.\n");
        return -1;
    }
    infer->forward(); // infer只能调用forward，也只能看见forward
    return 0;
}
```

infer.hpp文件：

```C++
#ifndef INFER_HPP
#define INFER_HPP
#include <memory>
#include <string>

// 头文件中的include干净整洁
// 只给外接提供create_infer接口，利用虚基类，让外接只能调用forward方法
class InferInterface
{
public:
    virtual void forward() = 0;
};
std::shared_ptr<InferInterface> create_infer(const std::string &file);

#endif
```

infer.cpp文件：

```C++
#include <string>
#include <memory>
#include "infer.hpp"

using namespace std;

class InferImpl : public InferInterface // 继承虚基类，从而实现load_model和destroy的隐藏
{
public:
    bool load_model(const string &file)
    {
        context_ = file;
        cout << "模型加载成功！" << endl;
        return true;
    }
    virtual void forward() override
    {
        printf("使用%s进行推理", context_.c_str());
    }
    void destroy()
    {
        context_.clear();
    }

private:
    string context_;
};

shared_ptr<InferInterface> create_infer(const string &file) // 返回的指针向虚基类转化
{
    shared_ptr<InferImpl> instance(new InferImpl());
    if (!instance->load_model(file))
    {
        instance.reset(); // 如果模型加载失败，instance要reset成空指针
    }
    return instance;
}
```

# 五、生产者消费者+RAII+接口模式综合案例

infer_test（抖音）

[抖音视频讲解](https://www.douyin.com/discover?modal_id=7076120030534028551)

[代码流程图（非常直观)   需下载使用亿图图示打开看](https://drive.google.com/file/d/1iNJkv2t7wEUaowFKIQRqioIpvEq_Num4/view?usp=drive_link)

```c++
#include "infer.hpp"
#include <thread>
#include <vector>
#include <condition_variable>
#include <mutex>
#include <string>
#include <future>
#include <queue>
#include <functional>

// 封装接口类
using namespace std;

struct Job{
  	// 定义一个Job结构体，结构体重必须有promise成员变量，用于每次生产者往队列中push任务的时候，能够有一个可以实例化的任务
    shared_ptr<promise<string>> pro;
    string input;
};

class InferImpl : public Infer{
public:
    virtual ~InferImpl(){
      	// 只有在析构实例的时候，才会退出子线程
        stop();
    }

    void stop(){
        if(running_){
          	// 将running_设置为false再通知子线程，从而使子线程退出
            running_ = false;
            cv_.notify_one();
        }
        if(worker_thread_.joinable())
          	// 子线程必须join，否则执行会报错
            worker_thread_.join();
    }

    bool startup(const string& file){

        file_ = file;
        running_ = true; // 启动后，运行状态设置为true

        // 线程传递promise的目的，是获得线程是否初始化成功的状态
        // 而在线程内做初始化，好处是，初始化跟释放在同一个线程内
        // 代码可读性好，资源管理方便
        promise<bool> pro;
        worker_thread_ = thread(&InferImpl::worker, this, std::ref(pro));   // 在给线程传引用的时候，只能用std::ref传递。因为线程中要改变pro的值，所以必须传引用
        /* 
            注意：这里thread 一构建好后，worker函数就开始执行了
            第一个参数是该线程要执行的worker函数，第二个参数是this指的是class InferImpl，第三个参数指的是传引用，因为我们在worker函数里要修改pro。
         */
        printf("第一次执行get_future");
      	// 等待子线程加载模型成功与否的结果返回
        return pro.get_future().get();
    }

    void worker(promise<bool>& pro){
        /* 
        建议先阅读代码，若有疑问，可点击抖音短视频进行辅助讲解(建议1.5倍速观看)
            worker函数 https://v.douyin.com/NfJPojm/
         */

        // load model
        if(file_ != "trtfile"){
            // failed
          	// 给pro.get_future().get()传递值
            pro.set_value(false);
            printf("Load model failed: %s\n", file_.c_str());
            return;
        }

        // load success
        pro.set_value(true); // 这里的promise用来负责确认infer初始化成功了
        printf("开始执行worker了");
        vector<Job> fetched_jobs;
        while(running_){
            {
                unique_lock<mutex> l(lock_);
                cv_.wait(l, [&](){return !running_ || !jobs_.empty();}); // 一直等着，cv_.wait(lock, predicate) // 如果 running不在运行状态 或者说 jobs_有东西 而且接收到了notify one的信号

                if(!running_) break; // 如果 不在运行 就直接结束循环
                
                int batch_size = 5;
                for(int i = 0; i < batch_size && !jobs_.empty(); ++i){   // jobs_不为空的时候
                    fetched_jobs.emplace_back(std::move(jobs_.front())); // 就往里面fetched_jobs里塞东西
                    jobs_.pop();                                         // fetched_jobs塞进来一个，jobs_那边就要pop掉一个。（因为move）
                }
            }

            // 一次加载一批，并进行批处理
            // forward(fetched_jobs)
            for(auto& job : fetched_jobs){
                job.pro->set_value(job.input + "---processed");
            }
            fetched_jobs.clear();
        }
        printf("Infer worker done.\n");
    }
  
  
    virtual shared_future<string> commit(const string& input) override{
        /* 
        建议先阅读代码，若有疑问，可点击抖音短视频进行辅助讲解(建议1.5倍速观看)
            commit 函数 https://v.douyin.com/NfJvHxm/
         */
        Job job;
        job.input = input;
        job.pro.reset(new promise<string>());

        shared_future<string> fut = job.pro->get_future();
        {
            lock_guard<mutex> l(lock_);
            jobs_.emplace(std::move(job));
        }
        cv_.notify_one();
      	/* 这里应该return fut而不应该return fut.get()，因为fut.get()会一直等，假如执行出错，fut.get()一直等不到,
      	   那么，就会在这里一直卡住 */
        return fut;
    }
private:
    atomic<bool> running_{false};
    string file_;
    thread worker_thread_;
    queue<Job> jobs_;
    mutex lock_;
    condition_variable cv_;
};

shared_ptr<Infer> create_infer(const string& file){
    /* 
        [建议先阅读代码，若有疑问，可点击抖音短视频进行辅助讲解(建议1.5倍速观看)]
        RAII+封装接口模式：问题定义-异常流程处理 https://v.douyin.com/NfJtnpF/
        RAII+封装接口模式：解决方案-用户友好设计 https://v.douyin.com/NfJteyc/
     */
    shared_ptr<InferImpl> instance(new InferImpl()); // 实例化一个推理器的实现类（inferImpl），以指针形式返回 
    if(!instance->startup(file)){                    // 推理器实现类实例(instance)启动。这里的file是engine file
        instance.reset();                            // 如果启动不成功就reset
    }
    return instance;    
}

void infer_test(){
    auto infer = create_infer("trtfile"); // 创建及初始化 抖音网页短视频辅助讲解: 创建及初始化推理器 https://v.douyin.com/NfJvWdW/
    if(infer == nullptr){                       
        printf("Infer is nullptr.\n");          
        return;
    }

    printf("commit msg = %s\n", infer->commit("msg").get().c_str()); // 将任务提交给推理器（推理器执行commit），同时推理器（infer）也等着获取（get）结果
}
```









